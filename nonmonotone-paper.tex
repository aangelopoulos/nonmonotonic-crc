\documentclass{article}

%\def\TimesFont{} % uncomment to enable times font
%\def\ParSkip{} % uncomment to enable paragraph skip
\input{macros}
\usepackage{centernot} % Center the "\not" over symbols 
\usepackage{mathtools}
\usepackage{coffeestains}
\def\notimplies{\centernot\implies} 
\def\Regret{\mathrm{Regret}}
\def\ana#1{\textcolor{red}{[ANA: #1]}}
\def\Bias{\mathrm{Bias}}
\def\thetahat{\hat{\theta}}
\def\gap#1{\mathsf{Gap}\left(#1\right)}
\def\overargmin{\overline{\arg \min}}
\def\A{\mathcal{A}}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\snap}{snap}

\mathtoolsset{showonlyrefs}


\title{Conformal Risk Control for Non-Monotonic Losses}
\author{Anastasios N.\ Angelopoulos\\\texttt{angelopoulos@berkeley.edu}}
\date{University of California, Berkeley\\Department of Electrical Engineering and Computer Sciences\\~\\ \today}

\begin{document}
\maketitle

\begin{abstract}
    Conformal risk control is an extension of conformal prediction for controlling risk functions beyond coverage.
    The original algorithm provides risk control guarantees only for monotone losses.
    Here, we present risk control guarantees for non-monotone losses as well.
    The guarantees depend on the stability of the risk control algorithm---unstable algorithms have looser guarantees. 
    We give applications of this technique to selective classification.
\end{abstract}

\section{Introduction}

Consider a sequence of random variables $D_{1:n+1} = ((X_1, Y_1), \ldots, (X_{n+1}, Y_{n+1})) \in (\cX \times \cY)^{n+1}$ with an arbitrary joint distribution $\P$.
We must predict $Y_{n+1}$ given access to a calibration dataset comprising the first $n$ datapoints, $D_{1:n}$, and the test covariate, $X_{n+1}$.
We will also define a bounded \emph{loss}, $\ell : \cX \times \cY \times \R^d \to [0,1]$.
The goal is to provide \emph{risk control} guarantees:
\begin{equation}
    \label{eq:intro-guarantee}
    \E\left[ \ell(X_{n+1}, Y_{n+1}; \thetahat) \right] \leq \alpha, 
\end{equation}
where $\thetahat \in \R^d$ is a parameter chosen by our algorithm using the calibration data and $\alpha \in [0,1]$ is user-specified.

Conformal risk control, a generalization of conformal prediction~\cite{gammerman1998learning, vovk1999machine, vovk2005algorithmic,lei2013conformal,lei2018distribution} as developed in~\cite{angelopoulos2024conformal, angelopoulos2024note}, handles the case where $d=1$ and $\ell$ is monotone decreasing in $\theta$.
Conformal risk control works by setting the parameter $\hat\theta$ to be
\begin{equation}
    \hat\theta = \inf\left\{ \theta : \frac{1}{n+1}\sum_{i=1}^{n}\ell(X_i,Y_i;\theta) \leq \alpha - \frac{1}{n+1} \right\},
\end{equation}
which is the smallest value of $\theta$ such that the risk is certain to be below $\alpha$ on all $n+1$ datapoints. This provides the guarantee in~\eqref{eq:intro-guarantee} for monotone losses, but can fail arbitrarily badly for non-monotone losses, as shown in Proposition 1 of~\cite{angelopoulos2024conformal}.

In this paper, we give risk control guarantees for non-monotone losses.
The key insight is that the population risk of $\thetahat$ depends on the stability of the algorithm for choosing $\thetahat$---i.e., the sensitivity of the empirical risk of $\thetahat$ to permutations of the full dataset.
More formally, let $\A : \cZ^* \to \R^d$ be an \emph{algorithm} mapping datasets to choices of $\theta$ and let $D_{-i}$ denote $D_{1:n+1}$ with the $i$th element removed.
The parameter $\thetahat$ is the output of $\A(D_{1:n})$.
We say a symmetric algorithm $\A$ is $\beta$-stable with respect to $\A^*$ and $\ell$ if
\begin{equation}
        \E\left[\frac{1}{n+1}\sum\limits_{i=1}^{n+1} \ell(X_{i}, Y_{i}; \A(D_{-i}))\right] 
        \leq \E\left[\frac{1}{n+1}\sum\limits_{i=1}^{n+1} \ell(X_{i}, Y_{i}; \A^*(D_{1:n+1}))\right] + \beta.
\end{equation}
The main theorem demonstrates that if a symmetric algorithm is stable with respect to an algorithm that controls the risk when run on the full data, then the original algorithm also controls the risk.
\begin{theorem}[Main result]
    \label{thm:main-intro}
    Assume $\A$ is symmetric and $\beta$-stable with respect to $\A^*$ and that
\begin{equation}
        \E\left[\ell(X_{n+1}, Y_{n+1}; \A^*(D_{1:n+1})) \right] \leq \alpha-\beta.
\end{equation}
    Then,
    \begin{equation}
        \E\left[\ell(X_{n+1}, Y_{n+1}; \A(D_{1:n})) \right] \leq \alpha.
    \end{equation}
\end{theorem}
\begin{proof}
Let $Z_i=(X_i,Y_i)$ and $Z_{1:n}=(Z_1,\ldots,Z_n)$.
By exchangeability and symmetry,
\[
\mathbb{E}\!\left[\frac{1}{n+1}\sum_{i=1}^{n+1}\ell(Z_i;\mathcal{A}(Z_{1:n+1}))\right]
= \mathbb{E}\!\left[\ell(Z_{n+1};\mathcal{A}(Z_{1:n+1}))\right],
\]
and
\[
\mathbb{E}\!\left[\frac{1}{n+1}\sum_{i=1}^{n+1}\ell(Z_i;\mathcal{A}(Z_1,\ldots,Z_{i-1},Z_{i+1},\ldots,Z_{n+1}))\right]
= \mathbb{E}\!\left[\ell(Z_{n+1};\mathcal{A}(Z_{1:n}))\right].
\]
By $\beta$-stability, $\mathbb{E}\!\left[\ell(Z_{n+1};\mathcal{A}(Z_{1:n}))\right] \leq \mathbb{E}\!\left[\ell(Z_{n+1};A_{1:n+1})\right] + \beta$.
Combining this with the assumption that $\mathbb{E}[\ell(Z_{n+1};A_{1:n+1})] \leq \alpha-\beta$ yields the result.
\end{proof}
Notice that Theorem~\ref{thm:main-intro} applies to $\theta$ in any space, and that boundedness of $\ell$ was never needed in the proof of Theorem~\ref{thm:main-intro}.
These concerns will come up later in determining stability bounds, building on the literature on algorithmic stability~\cite{kearns1997algorithmic, bousquet2002stability, kutin2002almost, mukherjee2006learning, shalev-shwartz2010learnability, hardt2016train, feldman2018generalization, bousquet2020sharper}; see Chapter 13 of~\cite{shalev-shwartz2014understanding} for a review of this extensive field.

\subsection{Conformal Risk Control is Stable for Monotonic Losses}

Here we will show that the conformal risk control algorithm is stable for monotonic losses, and thus, satisfies the guarantee in Theorem~\ref{thm:main-intro}.

\begin{proposition}
    \label{prop:crc-stable}
    For any dataset $D \in (\cX\times\cY)^*$, let $\ell$ be monotone in its last argument,
    \begin{equation}
        \A(D) = \inf\left\{ \theta : \frac{1}{|D|+1}\sum_{(x,y) \in D}\ell(x,y;\theta) \leq \alpha - \frac{1}{|D|+1} \right\}
    \end{equation}
    and
    \begin{equation}
        \A^*(D) = \inf\left\{ \theta : \frac{1}{|D|}\sum_{(x,y) \in D}\ell(x,y,\theta) \leq \alpha \right\}.
    \end{equation}
    Then $\A$ is $0$-stable with respect to $\A^*$.
\end{proposition}
\begin{proof}
    The boundedness of $\ell$ and definition of $\A$ imply that for all $i$,
    \begin{equation}
        \frac{1}{n+1}\sum_{i=1}^{n+1}\ell(X_i,Y_i,\A(D_{-i})) \leq \frac{1}{n+1}\sum_{i=1}^{n}\ell(X_i,Y_i,\A(D_{-i})) + \frac{1}{n+1} \leq \alpha.
    \end{equation}
    $\A(D_{-i})$ therefore satisfies the constraint in the definition of $\A^*(D_{1:n+1})$, and the monotonicity of the loss gives $\A(D_{-i}) \geq \A^*(D_{1:n+1})$. Therefore, $\ell(X_{i}, Y_{i}; \A(D_{-i})) \leq \ell(X_{i}, Y_{i}; \A^*(D_{1:n+1}))$, and
    \begin{equation}
        \E\left[\frac{1}{n+1}\sum\limits_{i=1}^{n+1} \Big(\ell(X_{i}, Y_{i}; \A(D_{-i})) - \ell(X_{i}, Y_{i}; \A^*(D_{1:n+1}))\Big)\right] \leq 0.
    \end{equation}
    Rearranging terms gives $0$-stability.
\end{proof}

Combining Proposition~\ref{prop:crc-stable} with Theorem~\ref{thm:main-intro} exactly recovers the classical conformal risk control guarantee.
\begin{corollary}
    The conformal risk control algorithm $\A$ defined in Proposition~\ref{prop:crc-stable} satisfies
    \begin{equation}
        \E\left[ \ell(X_{n+1}, Y_{n+1}; \A(D_{1:n}) \right] \leq \alpha
    \end{equation}
\end{corollary}
\begin{proof}
    Under monotone losses, Proposition~\ref{prop:crc-stable} shows that the conformal risk control algorithm $\mathcal{A}$ is $0$-stable. Applying Theorem~\ref{thm:main-intro} with $\beta=0$ recovers the guarantee.
\end{proof}

The key takeaway is that the proof of conformal risk control's validity has two parts, which can be decoupled: (1) proving monotonicity implies stability, and (2) proving that stability implies risk control.
The remainder of the paper will be dedicated to replacing step (1) with other stable algorithms, in the context of non-monotone losses, to provide risk control guarantees.
Looking forward, Section~\ref{sec:methods} gives stability bounds for classes of non-monotone losses and shows how to estimate these stability bounds for use in Theorem~\ref{thm:main-intro}. Section~\ref{sec:experiments} shows experimental validation of these methods.

\section{Methods}
\label{sec:methods}

\subsection{Setup and Notation}
\label{sec:setup}

\emph{Datasets} are arbitrary length ordered vectors of feature-label pairs, denoted $D \in \cZ^{*}$, where $\cZ = \cX \times \cY$, .
A dataset $D$ can have repeated entries, and is ordered; the size of the data set is denoted as $|D|$, and we can concisely denote sums over the dataset as $\sum_{(x,y) \in D}$, where the sum is over all elements of the dataset, and when the dataset contains repeated entries, they are counted multiple times (i.e., the sum is not over the set of unique elements of the dataset, but over all elements).
Finally, we will use $D_{1:k}$ to denote the first $k$ entries of the dataset $D$.

We will also use the notation $[k] = \{ 1, \ldots, k \}$ for any positive integer $k$, and $\Sigma$ to denote the set of all permutations over $[n+1]$.
For any $D \in \cZ^{n+1}$, we will let $D_\sigma$ denote the dataset obtained by permuting the elements of $D$ according to the permutation $\sigma \in \Sigma$.
Similarly, we will let $D_{\sigma(1:k)}$ denote the first $k$ elements of the permuted dataset $D_\sigma$ and $D_{-i}$ denote the dataset obtained by removing the $i$th element of $D$. 
When we need to work with the \emph{values} of the dataset $D_{1:n+1}$, agnostically to their original order, we let $E = ( (x_1,y_1), \ldots, (x_{n+1},y_{n+1}) )$ denote the dataset containing the values of $D_{1:n+1}$ in lexicographical order.
Any fixed ordering of the values will suffice, and we choose lexicographical ordering only for convenience.
We will let $\hat{R}_{D}(\theta) = \frac{1}{|D|}\sum_{(x,y) \in D}\ell(x,y,\theta)$ denote the empirical risk and $\thetahat_k = \A(D_{1:k})$ for all $k$.
Finally, when $\A$ is $\beta$-stable with respect to itself and $\ell$ is clear from context, we will simply say it is $\beta$-stable.

\subsection{Stable Algorithms}

This section describes three stable classes of risk control algorithms.
We begin with a narrow case study: selective prediction, otherwise known as prediction with abstention.
By abstaining from prediction on the hardest examples, selective prediction allows us to improve a model's accuracy on the event that it issues a prediction.
Selective prediction is one of the most important applications of non-monotone conformal risk control due to its ubiquitous practical utility in safe automation.
Next, we handle the case of risk control for general losses.
Our analysis will build upon and extend that of selective prediction, handling losses with any structure. 
The algorithm will work by discretizing $\theta$-space and then ensuring the resulting choice is stable.
Finally, we will handle the case of continuous, Lipschitz losses.

\subsubsection{Selective Prediction}

Let $\hat{Y}_i$ represent a prediction of $Y_i$ and $\hat{P}_i \in [0,1]$ represent a confidence in this prediction. Though this is elided in the notation, both $\hat{Y}_i = \hat{y}(X_i)$ and $\hat{P}_i = \hat{p}(X_i)$ are outputs of deterministic models run on $X_i$. We seek a confidence threshold $\thetahat$ satisfying
\begin{equation}
    \P(\hat{Y}_{n+1} \neq Y_{n+1} \mid \hat{P}_{n+1} > \thetahat) \leq \alpha.
\end{equation}
This is equivalent to asking that
\begin{equation}
    \E\left[ \ind{\hat{Y}_{n+1} \neq Y_{n+1} \text{ and } \hat{P}_{n+1} > \thetahat} - \alpha \ind{\hat{P}_{n+1} > \thetahat} + \alpha \right] = \E\left[ \ell(Z_{n+1};\thetahat) \right] \leq \alpha,
\end{equation}
where $\ell(x,y;\theta) = \ind{\hat{y}(x) \neq y \text{ and } \hat{p}(x) > \theta} - \alpha \ind{\hat{p}(x) > \theta} + \alpha$.
Notably, $\ell$ is a non-monotonic loss.
We will study the below algorithm $\A$ for selective prediction,
\begin{equation}
    \label{eq:leftmost-root}
    \A(D) = \inf\left\{ \theta : \frac{1}{|D|}\sum_{(x,y) \in D}\ell(x,y;\theta) \leq \alpha \right\},
\end{equation}
which simply chooses the smallest value of $\theta$ that controls the risk.

First, we characterize the stability of this selective prediction algorithm.
Let the vector $V$ be the indexes of the order statistics; that is, $\hat{P}_{V_1}=\hat{P}_{(1)}$, $\hat{P}_{V_2}=\hat{P}_{(2)}$, and so on, where $\hat{P}_{(j)}$ denotes the $j$th largest value of $\hat{P}_{1}, \ldots, \hat{P}_{n+1}$. 
Further, let $V^==\{i \in V : \hat{Y}_i = Y_i \}$, $V^{\neq} =\{i \in V : \hat{Y}_i \neq Y_i \}$, and $w_i = \alpha$ for $i \in V_{=}$ and $-(1-\alpha)$ otherwise.
The core idea is that each loss only has one change point, exactly at the location of $\hat{P}_i$:
\begin{align}
    \label{eq:selective-eq-neq}
    \ell(Z_i ; \theta) = 1 - (1-\alpha)\ind{\theta \geq \hat{P}_i} \qquad & \forall i \in V^{\neq}, \text{ and }\\
    \ell(Z_i ; \theta) = \alpha\ind{\theta \geq \hat{P}_i} \qquad & \forall i \in V^=.
\end{align}
Because the losses are right-continuous and piecewise constant with jumps at $\hat{P}$, we are guaranteed that if $D$ is a subdataset of $D_{1:n+1}$, then $\A(D)=\hat{P}_{(j)}$ for some $j$. 
Along these lines, define $j^*$ and $\hat{\jmath}_{-i}$ as
\begin{equation}
    \hat{P}_{(j^*)} = \A(D_{1:n+1}) \text{ and } \hat{P}_{(\hat{\jmath}_{-i})} = \A(D_{-i}).
\end{equation}

For a more direct (and efficiently computable) description of $j^*$ and $\hat{\jmath}_{-i}$, we can rewrite the selective prediction algorithm directly in index-space: that is, $j^* = \A'(D_{1:n+1})$, where
\begin{align}
    \A'(D_{1:n+1}) &= \max
    \left\{j : \sum_{v \in V^{=}}\alpha \ind{j \leq v} + \sum_{v \in V^{\neq}}(1-(1-\alpha)\ind{j \leq v}) \leq (n+1)\alpha \right\} \\
    &= \max
    \left\{j \in \{0, \ldots, n+1\} : 1+\sum_{i \leq j} w_i \geq 0 \right\}.
\end{align}
The leave-one-out index $\hat{\jmath}_{-i}=\mathcal{A}'(D_{-i})$ is defined similarly, with $V^=_{-i}$ in place of $V^=$, $V^{\neq}_{-i}$ in place of $V^{\neq}$, and $n$ in place of $n+1$.

\begin{proposition}
    \label{prop:stability-selective}
    The selective prediction algorithm in~\ref{eq:leftmost-root} is $\beta$-stable with
    \begin{equation}
        \beta = \frac{2\max\{\alpha, 1-\alpha\}\E[K]}{n+1}, \text{ where } K = \max_i|\hat{\jmath}_{-i} - j^*|.
    \end{equation}
\end{proposition}
\begin{proof}
    By~\eqref{eq:selective-eq-neq}, it suffices to prove that, almost surely,
    \begin{equation}
        \sum\limits_{i=1}^{n+1} \Big(\ell(X_{i}, Y_{i}; \A(D_{-i})) - \ell(X_{i}, Y_{i}; \A^*(D_{1:n+1}))\Big) = \sum_{i\in [n+1]} w_i\Delta_i
        \leq 2(1-\alpha)K,
    \end{equation}
    where $\Delta_i = \ind{\A(D_{-i}) \geq \hat P_i} - \ind{\A^*(D_{1:n+1}) \geq \hat P_i}.$
    We can begin by rewriting the $\Delta_i$ terms as 
    \begin{align}
        \Delta_i &= \ind{\A(D_{-i}) \geq \hat{P}_i > \A(D_{1:n+1})} - \ind{\A(D_{-i}) < \hat{P}_i \leq \A(D_{1:n+1})} \\
        &= \ind{\hat{\jmath}_{-i} \leq V_i < j^*} - \ind{\hat{\jmath}_{-i} > V_i \geq j^*}.
    \end{align}
    Note that the two events inside the indicators are mutually exclusive, and correspond to the ``crossing'' of $\A(D_{-i})$ over $\hat{P}_i$ in the downward or upward directions.
    Therefore,
    \begin{equation}
        \label{eq:pf-selective-cauchy-schwarz}
        \sum_{i\in [n+1]} w_i\Delta_i \leq \max\{\alpha, 1-\alpha\} \sum_{i\in [n+1]}|\Delta_i| \leq \max\{\alpha, 1-\alpha\}\left|\{i : \hat{\jmath}_{-i} \leq V_i < j^* \text{ or } \hat{\jmath}_{-i} > V_i \geq j^*\} \right|.
    \end{equation}
    Define the index sets
\[
S_- \;:=\; \bigl\{i\in[n{+}1]:\ \hat{\jmath}_{-i}\le V_i<j^*\bigr\}
\quad\text{and}\quad
S_+ \;:=\; \bigl\{i\in[n{+}1]:\ \hat{\jmath}_{-i}> V_i\ge j^*\bigr\}.
\]
Then the set of indices that contribute nonzero terms to $\sum_{i\in[n+1]}|\Delta_i|$ is $S:=S_-\cup S_+$, and $S_-\cap S_+=\varnothing$ by definition.

By the definition of $K=\max_i|\hat{\jmath}_{-i}-j^*|$, for every $i\in S_-$ we have
\[
1 \le j^*-V_i \le j^*-\hat{\jmath}_{-i}\le |\hat{\jmath}_{-i}-j^*|\le K,
\]
hence
\[
V_i\in\{j^*-K,\ldots,j^*-1\}\cap\{1,\ldots,n{+}1\}.
\]
Similarly, for every $i\in S_+$ we have $j^*\le V_i\le \hat{\jmath}_{-i}-1\le j^*+K-1$, i.e.
\[
V_i\in\{j^*,\ldots,j^*+K-1\}\cap\{1,\ldots,n{+}1\}.
\]
Because the $V_i$â€™s are distinct ranks (the map $i\mapsto V_i$ is a permutation of $\{1,\ldots,n{+}1\}$), at most one index $i$ can correspond to each admissible value of $V_i$. Therefore,
\[
|S_-|\le K\qquad\text{and}\qquad |S_+|\le K,
\]
which, since $|\Delta_i| \leq 1$ for all $i$, yields
\[
\sum_{i\in[n+1]}|\Delta_i| \leq |S| \leq |S_-|+|S_+| \leq 2K.
\]
Combining this with~\eqref{eq:pf-selective-cauchy-schwarz} yields
\[
\sum_{i\in[n+1]} w_i\Delta_i \;\le\; \max\{\alpha, 1-\alpha\}\,|S| \;\le\; 2\max\{\alpha, 1-\alpha\}K,
\]
as claimed.
\end{proof}

The above result gives a simple, distribution-free characterization of the stability of the selective prediction algorithm.
It is then relatively easy to estimate $\E[K]$ with the bootstrap---i.e., drawing samples from the empirical distribution, computing $K$ on these samples, and taking the average.

However, the question remains as to the scaling of $\E[K]$ with $n$.
The following proposition gives an answer, via an explicit characterization of $\E[K]$.
\begin{proposition}
    \label{prop:ub-EK}
    In the same setting as Proposition~\ref{prop:stability-selective}, define $\bar{E}_j = \frac{1}{j}\sum_{i \leq j}E_i$, where
    \begin{equation}
        E = \left(\ind{\hat{Y}_{V_1} \neq Y_{V_1}}, \ldots, \ind{\hat{Y}_{V_{n+1}} \neq Y_{V_{n+1}}}\right)
    \end{equation}
    Then,
    \begin{equation}
        \E[K] \leq \sum_{j=1}^{n+1} \left[\P\left(\bar{E}_j \in \bigg(\alpha + \frac{1-\alpha}{j}, \alpha + \frac{2-\alpha}{j}\bigg] \right)\right].
    \end{equation}
\end{proposition}

\begin{proof}
    We keep the same notation as in the proof of Proposition~\ref{prop:stability-selective},
    and notice that 
    \begin{equation}
        K = \max_i |\A'(D_{-i})-\A'(D_{1:n+1})|.
    \end{equation}
    Therefore, $K$ depends on the values of $\hat{P}_1, \ldots, \hat{P}_{n+1}$, only through the error vector $E$ (which itself only depends on their order).
    In other words, we can write the partial sums $T_j = \sum_{i \leq j}w_i = -\sum_{i\leq j}E_i+j\alpha$, and by construction,
    \begin{equation}
        \mathcal{A}'(D_{1:n+1})=\max\big\{j \in \{0, \ldots, n+1\} :\;1+T_j\ge 0\big\}.
    \end{equation}
    If we delete index $i$, then for all $j\geq i$ the new partial sums shift by a constant:
    \begin{equation}
        T^{-i}_{j}=T_{j+1}-\alpha\quad\text{if }E_i=0,
        \qquad
        T^{-i}_{j}=T_{j+1}+(1-\alpha)\quad\text{if }E_i=1.
    \end{equation}
    Hence removing a correct point ($E_i=0$) can only invalidate those $j \geq i$ for which $1+T_{j+1}\in[0,\alpha)$, and removing an error ($E_i=1$) can only create feasibility for those $j \geq i$ with $1+T_{j+1}\in[-(1-\alpha),0)$. Consequently,
    \begin{equation}
        K \leq \bigl|\{j \in [n] :\ T_j\in[-1,\alpha-1)\}\bigr| + \bigl|\{j :\ T_j \in [\alpha-2, -1)\}\bigr| = \bigl|\{j \in [n] :\ T_j\in[\alpha-2,\alpha-1)\}\bigr|
    \end{equation}

    We can bound this quantity in expectation as
    \begin{align}
        \label{eq:EK-ub-1}
        &\E[K] \leq \E\big[\bigl|\{\{j \in [n] :\ T_j\in[\alpha-2,\alpha-1)\}\bigr|\big] \\
        = &\E\left[\sum\limits_{j=1}^{n+1} \ind{\alpha-2 \leq T_j < \alpha-1}\right] = \sum\limits_{j=1}^{n+1} \P\left(T_j \in [\alpha-2, \alpha-1) \right)=\sum\limits_{j=1}^{n+1}\P\left(\bar{E}_j \in \bigg(\alpha + \frac{1-\alpha}{j}, \alpha + \frac{2-\alpha}{j}\bigg] \right).
    \end{align}
\end{proof}

\subsubsection{Continuous, Lipschitz Losses}
Finally, we give a natural generalization of the conformal risk control algorithm that works under regularity conditions.
The idea is to choose the rightmost location where the risk crosses $\alpha$.
If this rightmost root is leave-one-out stable, meaning that removals of a single datapoint do not drastically change the location of the root, then for smooth losses, risk control will hold.

\begin{proposition}
    \label{prop:stability-smooth}
    Let $\A(D) = \sup\{\theta \in \Theta : \hat{R}_D(\theta) \leq \alpha\}$ for $\Theta = [\theta_{\rm min},\theta_{\rm max}]$.
    Furthermore, let $\ell$ be continuous and $L$-Lipschitz in $\theta$, and assume there exist $s \in \{-1, +1\}$ and $m,r,\gamma>0$ such that
    \begin{equation}
        \label{eq:stable_root}
        s(\hat{R}_{D_{1:n+1}}(\theta) - \alpha) > m(\theta - \thetahat_{n+1}) \; \; \forall \theta \in [\thetahat_{n+1} \pm r ] \quad \text{ and } \quad s(\hat{R}_{D_{1:n+1}}(\theta) - \alpha) > \gamma \; \; \forall \theta \in [\thetahat_{n+1}, \theta_{\rm max}]      
    \end{equation}
    Then if $\tfrac{1}{n+1} \leq \min\{mr,\gamma\}$, $\A$ is $\tfrac{L}{m(n+1)}$-stable with respect to $\A^*\equiv\A$.
\end{proposition}
\begin{proof}
    Write $R_{-i}:=\hat{R}_{D_{-i}}$ and $R:=\hat{R}_{D_{1:n+1}}$, and set $\varepsilon:=1/(n+1)$. 
    First, observe the following leave-one-out bound, whose proof is trivial:
    \begin{equation}
        \label{eq:loo-bound}
        \sup_{\theta\in\Theta}\,|R_{-i}(\theta)-R(\theta)|\ \leq  \epsilon.
    \end{equation}
    Combining the first assumption in~\eqref{eq:stable_root} and $R(\hat\theta_{n+1})=\alpha$,
    \begin{equation}
        s\!\left(R\!\left(\hat\theta_{n+1}+\delta\right)-\alpha\right)\ge m\delta \quad \text{ and } \quad
        s\!\left(R\!\left(\hat\theta_{n+1}-\delta\right)-\alpha\right)\le -m\delta \qquad \forall\,\delta\in[0,r].
    \end{equation}
    Applying the above with $\delta=\varepsilon/m$ gives
    \begin{equation}
        s\!\left(R_{-i}\!\left(\hat\theta_{n+1}+\tfrac{\varepsilon}{m}\right)-\alpha\right)\ge m\cdot\tfrac{\varepsilon}{m}-\varepsilon=0, \quad \text{ and } \quad s\!\left(R_{-i}\!\left(\hat\theta_{n+1}-\tfrac{\varepsilon}{m}\right)-\alpha\right)\le -m\cdot\tfrac{\varepsilon}{m}+\varepsilon=0,
    \end{equation}
    implying that $\alpha \in \left[ R_{-i}\!\left(\hat\theta_{n+1}\pm \tfrac{\varepsilon}{m}\right)\right]$.
    Therefore, by continuity, $R_{-i}(\theta)=\alpha$ has a solution in
    \[
    I:=\Big[\hat\theta_{n+1}-\tfrac{\varepsilon}{m},\ \hat\theta_{n+1}+\tfrac{\varepsilon}{m}\Big].
    \]
    Next, for any $\theta\in[\hat\theta_{n+1}+\varepsilon/m,\ \hat\theta_{n+1}+r]$, the first assumption in~\eqref{eq:stable_root} combines with~\eqref{eq:loo-bound} to yield
    \[
    s\!\left(R_{-i}(\theta)-\alpha\right)\ \ge\ m(\theta-\hat\theta_{n+1})-\varepsilon\ >\ 0,
    \]
    so there is no root of $R_{-i}(\theta)=\alpha$ in that interval. For $\theta\ge \hat\theta_{n+1}+r$, the first assumption in~\eqref{eq:stable_root} combines with~\eqref{eq:loo-bound} to give
    \[
    s\!\left(R_{-i}(\theta)-\alpha\right)\ \ge\ \gamma-\varepsilon\ >\ 0,
    \]
    so there is no root to the right of $\hat\theta_{n+1}+r$ either. Hence the \emph{rightmost} root of $R_{-i}(\theta)=\alpha$ lies in $I$, and therefore
    \[
    \big|\A(D_{-i})-\hat\theta_{n+1}\big|\ \le\ \frac{\varepsilon}{m}\ =\ \frac{1}{m(n+1)}.
    \]
    
    Finally, since $\ell(\cdot;\theta)$ is $L$-Lipschitz in $\theta$,
    \[
    \ell\!\left(Z_i;\A(D_{-i})\right)
    \le \ell\!\left(Z_i;\hat\theta_{n+1}\right)
    + L\,\big|\A(D_{-i})-\hat\theta_{n+1}\big|
    \le \ell\!\left(Z_i;\hat\theta_{n+1}\right)+\frac{L}{m(n+1)}.
    \]
    Averaging over $i=1,\dots,n+1$ yields the stated bound and completes the proof.
\end{proof}

\begin{corollary}
    Under the same conditions as Proposition~\ref{prop:stability-smooth}, $\A$ satisfies
    \begin{equation}
        \E\left[ \ell(X_{n+1}, Y_{n+1}; \A(D_{1:n}) \right] \leq \alpha + \frac{L}{m(n+1)}.
    \end{equation}
\end{corollary}

\subsubsection{General bounded losses}
\label{sec:general-bounded}
To handle general losses bounded in $[0,1]$, we will use algorithms that discretize $\Theta$ to impose stability.
First, let $\A(D) = \sup\{\theta \in \Theta_m = \{0, \tfrac{1}{m}, \tfrac{2}{m}, \ldots, 1\} : \hat{R}_D(\theta) \leq \alpha\}$ for $\Theta = [0,1]$.
This is simply the discretized version of the previously studied algorithms.
Since we have no assumptions on the loss, we will rely on the coarse stability bound
\begin{align}
    \E[R(\A(D_{-(n+1)})) - R(\A(D_{1:n+1}))] \leq \P(R(\A(D_{-(n+1)})) > \alpha + \epsilon, \hat{R}_n(D_{-(n+1)}) \leq \alpha ) + \epsilon, \quad \forall \epsilon \geq 0 .
\end{align}
The above bound essentially tells us that the stability depends on having no high-risk values of $\theta$ that could potentially be selected by the leave-one-out procedure.
The following proposition makes this intuition formal.
\begin{proposition}
\label{prop:general-stability}
Let $\ell(\cdot;\theta)\in[0,1]$ for all $\theta\in[0,1]$, fix $\Theta_m=\{0,\tfrac1m,\ldots,1\}$, let $\ell(\cdot;1)=0$, and define
\begin{equation}
\A(D)\ :=\ \sup\bigl\{\theta\in\Theta_m:\ \hat R_D(\theta)\le \alpha\bigr\}.
\end{equation}
Then,
\begin{equation}
\E\bigl[\ell(Z_{n+1};\A(D_{1:n}))\bigr]\ \leq\ \alpha+\epsilon^*+\frac{1}{4n\epsilon^*} \leq \alpha + \sqrt{\frac{e\ln(4n(m+1)^2)}{8(e-1)n}} + \frac{2}{\sqrt{n(\ln(4n(m+1)^2)-\ln(\ln(4n(m+1)^2)))}},
\end{equation}
where 
\begin{equation}
    \epsilon^* = 
    \sqrt{-\frac{W_{-1} \left( -\frac{1}{4n(m+1)^2}\right)}{8n}},
\end{equation}
and $W_{-1}$ is the $-1$st branch of the Lambert W function.
\end{proposition}
This proposition tells us that we can achieve risk control for general losses with the discretized algorithm up to a dominating factor of $\mathcal{O}\left(\sqrt{\tfrac{\ln(n(m+1))}{n}}\right)$.

\subsection{Guarantees for empirical risk minimization}
So far, we have studied root-finding algorithms that select the most extreme point satisfying an inequality.
Here, we will instead study regularized empirical risk minimization (ERM) algorithms: $\A(D) = \argmin_{\theta \in \R^d} \hat{R}_D(\theta) + \tfrac{\lambda}{2} \|\theta\|_2^2$.
We will let $\ell \in \R^d$ be potentially unbounded.

We will show two types guarantees with respect to ERM.
First, we prove guarantees on the generalization gap: the gap between the risk of the empirical risk minimizer with respect to an oracle quantity, such as the full-data or population risk minimizers.
Second, we prove stability bounds on the first-order optimality condition; i.e., how close the population gradient is to zero.
It is known that the standard empirical risk minimizer cannot be stable in either sense for all loss functions. 

\subsubsection{Risk control guarantees on the loss}
First, we show the stability of ERM on the scale of the loss.
\begin{proposition}
\label{prop:erm-stable-loss}
Let $\Theta=\R^d$, $\ell$ be convex in $\theta$, and assume there exists $\rho:\cZ\to[0,\infty)$ such that for all $z\in\cZ$ and $\theta,\theta'\in\R^d$,
\begin{equation}
\label{eq:lipschitz-rho-simple}
|\ell(z;\theta)-\ell(z;\theta')|\ \le\ \rho(z)\,\|\theta-\theta'\|_2.
\end{equation}
Then $\A$ is $\beta$-stable with
\begin{equation}
\label{eq:beta-simple}
\beta \leq \frac{2\,\E[\rho(Z_{n+1})^2]}{\lambda(n+1)}.
\end{equation}
\end{proposition}
This result is a minor variation on the canonical results in Section 5 of~\cite{bousquet2002stability}, and can be used in conjunction with Theorem~\ref{thm:main-intro} to give a generalization guarantee on ERM, as below.
\begin{corollary}
    In the setting of Proposition~\ref{prop:erm-stable-loss},
    \begin{equation}
        \E\left[ \ell(X_{n+1}, Y_{n+1}; \A(D_{1:n})) \right] \leq R^* + \frac{2\,\E[\rho(Z_{n+1})^2]}{\lambda(n+1)},
    \end{equation}
    where $R^* = \E[\ell(X_{n+1},Y_{n+1};\theta^*)]$ and $\theta^* = \argmin_{\theta \in \Theta} \E[\ell(X_{n+1},Y_{n+1};\theta)]$.
\end{corollary}
This guarantee finds use for controlling U-shaped risks, for example, the intersection-over-union:
\begin{equation}
    \ell(x,y;\theta) = 1-\frac{\cC_{\theta}(x) \cap y}{\cC_{\theta}(x) \cup y},
\end{equation}
where $\cC_{\theta}$ is a sequence of sets nested in $\theta$, $y$ is a set-valued response, and $x$ can be in any space.

\subsubsection{Risk control guarantees on the gradient}

Next, we address stability of the expected gradient.
Building towards this, we will need a different notion of stability to handle gradients when $d>1$: an algorithm $\cA$ is $\beta$-stable with respect to $\A^*$ and $g$ if
\begin{equation}
        \E\left[\frac{1}{n+1}\sum\limits_{i=1}^{n+1} g(X_{i}, Y_{i}; \A(D_{-i}))\right] 
        \preceq \E\left[\frac{1}{n+1}\sum\limits_{i=1}^{n+1} g(X_{i}, Y_{i}; \A^*(D_{1:n+1}))\right] + \beta,
\end{equation}
where $\beta \in \R^d$.
Theorem~\ref{thm:main-intro} also has an extension to $d>1$, which we present below.
\begin{theorem}[Multivariate extension of main result]
    \label{thm:main-high-d}
    Assume $\A$ is symmetric and $\beta$-stable with respect to $\A^*$ and $g$ and that
\begin{equation}
        \E\left[g(X_{n+1}, Y_{n+1}; \A^*(D_{1:n+1})) \right] \preceq \alpha-\beta.
\end{equation}
    Then,
    \begin{equation}
        \E\left[g(X_{n+1}, Y_{n+1}; \A(D_{1:n})) \right] \preceq \alpha.
    \end{equation}
\end{theorem}
The proof of this theorem is identical to that of Theorem~\ref{thm:main-intro}, except with $\preceq$ in place of $\leq$.
Now we proceed with the stability result, stated below for differentiable convex losses. The differentiability is not needed, and is only used to simplify the statement and proof of the proposition.
\begin{proposition}
\label{prop:erm-grad-stability}
Assume $\ell$ is a convex, differentiable function of $\theta$ and that there exists a measurable function $\rho : Z \to [0,\infty)$ such that for all $\theta,\theta' \in \R^d$ and almost all $z \in \cZ$,
\begin{equation}
\|\nabla\ell(z;\theta) - \nabla\ell(z;\theta')\|_2 \leq \rho(z)\|\theta - \theta'\|_2.
\end{equation}
Assume also that $\ell$ is $\mu$-strongly convex for some $\mu \in [0,\infty)$ (noting that $\mu=0$ is also allowable, so the loss need not be strongly convex).
Then, $\cA$ is $\beta$-stable with respect to $\nabla\ell$, where
\begin{equation}
\beta = \frac{\E[\rho(Z_{n+1})\|\nabla\ell(Z_{n+1}; \A(D_{1:n}))\|_2] + \E\left[\rho(Z_{n+1})\|\tfrac{1}{n}\sum_{j=1}^n\nabla\ell(Z_j;\A(D_{1:n}))\|_2\right]}{(\mu + \lambda)(n+1)} \mathbf{1}_d.
\end{equation}
Furthermore, under independence of $Z_1, \ldots, Z_{n+1}$, this simplifies to
\begin{equation}
\beta = \frac{\E[\rho(Z_{n+1})\|\nabla\ell(Z_{n+1};\A(D_{1:n}))\|_2] + \E\left[\rho(Z_{n+1})\right]\E\left[\|\tfrac{1}{n}\sum_{j =1}^n\nabla\ell(Z_j;\A(D_{1:n}))\|_2\right]}{(\mu + \lambda)(n+1)} \mathbf{1}_d.
\end{equation}
\end{proposition}
Eliding the Lipschitz constant $\rho(Z_{n+1})$, this result tells us that ERM is stable with respect to $\nabla\ell$, with
\begin{equation}
    \beta = \frac{\E[\text{magnitude of test gradient}] + \E[\text{magnitude of training risk}]}{(\mu + \lambda)(n+1)}.
\end{equation}
Under normal circumstances, both terms in the numerator will be constant-order, and the latter term will be near-zero (in fact, when $\lambda=0$, it is exactly zero).
Furthermore, we have stated $\beta$ in a way that it can be easily estimated from data, and therefore yields an actionable risk control algorithm.
Proposition~\ref{prop:erm-grad-stability} also holds for non-differentiable losses, replacing every gradient in $\beta$ with a supremum over all subgradients in the subdifferential at $\thetahat_{-(n+1)}$; the proof of this more general result is morally the same, so we omit it.

Combining Proposition~\ref{prop:erm-grad-stability} with Theorem~\ref{thm:main-high-d} gives a multi-dimensional risk control bound, as below.
\begin{corollary}
    \label{cor:regularized-erm}
    In the same setting as Proposition~\ref{prop:erm-grad-stability}, we have that
    \begin{equation}
        \E\left[\nabla\ell(X_{n+1}, Y_{n+1}; \A(D_{1:n})) \right] \preceq \beta - \lambda\E[\A(D_{1:n})].
    \end{equation}
\end{corollary}
This tells us the risk is controlled up to $\beta$ plus a term to handle regularization.

Finally, we present an adjusted algorithm that achieves a conservative risk control guarantee on the gradient.
Let
\begin{equation}
    \label{eq:erm-linear}
    \widetilde{\A}_\gamma(D) = \argmin_{\theta \in \R^d} \hat{R}_D(\theta) + \tfrac{\lambda}{2} \|\theta\|_2^2 - \gamma \mathbf{1}_d^{\top}\theta.
\end{equation}
The final term, $\gamma \mathbf{1}_d^{\top}\theta$, has gradient $-\gamma\mathbf{1}_d$, pushing all components of the solution in the negative direction, while retaining convexity.
A careful selection of $\gamma$ to balance terms can therefore yield risk control.
\begin{proposition}
    \label{prop:regularized-erm-conservative}
    In the same setting as Proposition~\ref{prop:erm-grad-stability}, we have that 
    \begin{equation}
        \E\left[\nabla\ell(X_{n+1}, Y_{n+1}; \A(D_{1:n})) \right] \preceq \beta_\gamma + \lambda\E[\A(D_{1:n})] - \gamma\mathbf{1}_d,
    \end{equation}
    where
    \begin{equation}
        \beta_{\gamma} = \frac{\E[\rho(Z_{n+1})\|\nabla\ell(Z_{n+1};\hat{\theta}_{-(n+1)})\|_2] + \E\left[\rho(Z_{n+1})\right]\left(\E\left[\|\tfrac{1}{n}\sum_{j =1}^n\nabla\ell(Z_j;\hat{\theta}_{-(n+1)})\|_2\right]+2\gamma\right)}{(\mu + \lambda) (n+1)} \mathbf{1}_d.
    \end{equation}
    Furthermore, provided $n > \tfrac{2}{\lambda} \E[\rho(Z_{n+1})] - 1$, setting 
    \begin{equation}
        \gamma = \frac{
        \displaystyle
        \frac{
        \E\left[\rho(Z_{n+1})\|\nabla\ell(Z_{n+1};\hat{\theta}_{-(n+1)})\|_2\right]
        +
        \E[\rho(Z_{n+1})]\,
        \E\left[\left\|\frac{1}{n}\sum_{j=1}^n \nabla\ell(Z_j;\hat{\theta}_{-(n+1)})\right\|_2\right]
        }{(\mu + \lambda)(n+1)}
        +
        \lambda\left\|\E[\A(D_{1:n})]\right\|_\infty
        }{
        \displaystyle
        1 - \frac{2\,\E[\rho(Z_{n+1})]}{(\mu + \lambda) (n+1)}
        }
    \end{equation}
    gives
    \begin{equation}
        \E\left[ \nabla\ell(X_{n+1}, Y_{n+1}; \A(D_{1:n})) \right] \preceq 0.
    \end{equation}
\end{proposition}

The main utility of these risk-control guarantees on the gradient is to derive distribution-free multi-group risk control guarantees.
These guarantees are in the style of~\cite{gibbs2023conformal} and~\cite{blot2024automatically}, which handle the special cases of the quantile loss and monotone losses in $d=1$ respectively, and both of which only give full-conformal type guarantees that require having access to the test covariate $X_{n+1}$ and potentially looping through all possible test labels $y \in \cY$.
Herein, we allow a more general class of non-monotone losses, a high-dimensional $\theta$, and also give split-conformal versions of these techniques that do not depend on the test covariate or label.

The first example shows how we can post-process a black-box model to achieve multigroup unbiasedness.
\begin{corollary}
    \label{cor:unbiased-least-squares}
    Let $\cX=\{0,1\}^d$, $f : \cX \to \R$ be any fixed function, $\ell(x,y;\theta) = \tfrac{1}{2}\|f(x) + x^\top\theta - y\|_2^2$, and $\cA(D) = \argmin_{\theta \in \R^d} \frac{1}{|D|}\sum_{(x,y)\in D}\ell(x,y;\theta)$.
    Then, for all $j \in [d]$,
    \begin{equation}
        \E[Y_{n+1} \mid X_{n+1,j} = 1] \in \left( \E[f(X_{n+1}) + \A(D_{1:n})_j \mid X_{n+1,j} = 1] \pm \frac{\E[|f(X_{n+1})+X_{n+1}^\top\A(D_{1:n})-Y_{n+1}|]}{n+1}\right).
    \end{equation}
\end{corollary}
If $X_{n+1}$ represents a vector of group membership indicators, the adjusted prediction $f(X_{n+1})+\cA(D_{1:n})$ is unbiased for all groups simultaneously.
This argument is the batch version of the gradient equilibrium guarantees in~\cite{angelopoulos2025gradient}.
Much like in the case of gradient equilibrium, Corollary~\ref{cor:regularized-erm} extends to all generalized linear models, not just linear regression, and we omit the proof because it is essentially identical.

\subsection{Estimating the stability parameter}
\label{sec:beta-estimation}
Here, we seek to estimate upper bounds on the stability parameter.
We will do so using the calibration sample, and averaging across bootstrap replicates. 
Let $\hat{\P}_n$ be the empirical measure on $D_{1:n}=\{Z_i=(X_i,Y_i)\}_{i=1}^n$. For $b=1,\ldots,B$, draw an i.i.d.\ bootstrap dataset
\[
D^{(b)}=\bigl(Z^{(b)}_1,\ldots,Z^{(b)}_{n+1}\bigr)\sim\hat{\P}_n^{\,n+1},
\]
compute the statistic of interest on $D^{(b)}$ (holding any fitted prediction rules, e.g.\ $\hat y,\hat p$, fixed), and aggregate by the mean:
\[
\overline{T}\;=\;\frac{1}{B}\sum_{b=1}^B T^{(b)}.
\]
Because $\ell\in[0,1]$ and $0\le K\le n{+}1$, all statistics below are bounded; thus, the bootstrap mean $\overline{T}$ consistently estimates the corresponding population expectation under standard regularity conditions as given in Chapter 3.6 of~\cite{vandervaart1996weak}.

\subsubsection{Directly from the definition of \texorpdfstring{$\beta$}{beta}}
Fix a reference algorithm $\A^*$ (as in Theorem~\ref{thm:main-intro}). For each bootstrap dataset $D^{(b)}$ compute
\begin{equation}
\label{eq:beta-def-bootstrap}
\Delta^{(b)}
\;:=\;
\frac{1}{n+1}\sum_{i=1}^{n+1}
\Bigl[\,
\ell\bigl(Z^{(b)}_i;\A(D^{(b)}_{-i})\bigr)
-
\ell\bigl(Z^{(b)}_i;\A^*(D^{(b)})\bigr)
\,\Bigr].
\end{equation}
The minimal admissible stability constant satisfies $\beta\ge \E[\Delta]_+$. We estimate it by the positive part of the bootstrap mean
\[
\widehat{\beta}_{\rm def} \;:=\; \bigl(\,\overline{\Delta}\,\bigr)_+,
\qquad
\overline{\Delta}=\frac{1}{B}\sum_{b=1}^B \Delta^{(b)}.
\]

\subsubsection{Selective prediction: directly from \texorpdfstring{$K$}{K}}
In the selective prediction setting, for each $D^{(b)}$ compute
\[
j^{*(b)}=\A'(D^{(b)}),\qquad
\hat{\jmath}^{(b)}_{-i}=\A'(D^{(b)}_{-i}),\qquad
K^{(b)}=\max_{i\in[n+1]}\bigl|\hat{\jmath}^{(b)}_{-i}-j^{*(b)}\bigr|.
\]
Proposition~\ref{prop:stability-selective} gives
\(
\beta \le \tfrac{2\max\{\alpha,1-\alpha\}}{n+1}\,\E[K].
\)
Estimate $\E[K]$ by $\overline{K}=\frac{1}{B}\sum_b K^{(b)}$ and set
\[
\widehat{\beta}_{K} \;:=\; \frac{2\max\{\alpha,1-\alpha\}}{n+1}\,\overline{K}.
\]

\subsubsection{Selective prediction: distribution-free nonasymptotic bound}
Proposition~\ref{prop:ub-EK} yields
\(
\E[K] \le \sum_{j=1}^{n+1}\P\!\left(\bar{E}_j \in I_j\right),
\)
where $\bar{E}_j=\frac{1}{j}\sum_{i\le j}E_i$ and
\(
I_j=\bigl(\alpha+\tfrac{1-\alpha}{j},\;\alpha+\tfrac{2-\alpha}{j}\bigr].
\)
For each bootstrap dataset, form $E^{(b)}$ by sorting examples in decreasing $\hat P$ and recording $E^{(b)}_i=\ind{\hat Y\neq Y}$; then compute Boolean variables
\(
J^{(b)}_{j}=\ind{\bar{E}^{(b)}_{j}\in I_j}.
\)
Average across bootstrap replicates to estimate the bound:
\[
\widehat{\E[K]}_{\rm df} \;:=\; \frac{1}{B}\sum_{b=1}^B \sum_{j=1}^{n+1} J^{(b)}_{j},
\qquad
\widehat{\beta}_{\rm df} \;:=\; \frac{2\max\{\alpha,1-\alpha\}}{n+1}\,\widehat{\E[K]}_{\rm df}.
\]

\subsubsection{Smooth losses: Lipschitz--slope bound}
Under the assumptions of Proposition~\ref{prop:stability-smooth}, $\beta\le L/(m(n+1))$. We estimate $L$ and the local slope $m$ on each bootstrap dataset and average the resulting bounds.

Fix a grid $\Theta_G\subset\Theta$. For $D^{(b)}$, let
\begin{equation}
    \widehat{L}^{(b)} \;:=\; \sup_{(x,y)\in D^{(b)}}\ \sup_{\theta\neq\theta'\in\Theta_G}
\frac{|\ell(x,y;\theta)-\ell(x,y;\theta')|}{|\theta-\theta'|}.
\end{equation}

Let $\hat\theta^{(b)}=\A(D^{(b)})$ and $\hat R^{(b)}(\theta)=\frac{1}{n+1}\sum_{(x,y)\in D^{(b)}}\ell(x,y;\theta)$. 
Define a slope near the rightmost root:
\begin{equation}
    \widehat{m}^{(b)} \;:=\; \inf_{\delta\in[\delta_{\min},\delta_{\max}]}
    \frac{\left|\bigl(\hat R^{(b)}(\hat\theta^{(b)}+\delta)-\hat R^{(b)}(\hat\theta^{(b)})\bigr)\right|}{\delta},
\end{equation}
and, with $s=\mathrm{sign}(\hat R^{(b)}(\hat\theta^{(b)})-\alpha)$, define the empirical margins
\begin{equation}
    \widehat{r}^{(b)}:=\sup\{\delta:\ s(\hat R^{(b)}(\hat\theta^{(b)}+t)-\alpha)\ge \widehat{m}^{(b)}t,\ \forall t\in[0,\delta]\},\qquad
    \widehat{\gamma}^{(b)}:=\inf_{\theta\in[\hat\theta^{(b)},\theta_{\max}]}\ s(\hat R^{(b)}(\theta)-\alpha).
\end{equation}

Define the per-replicate bound as
\[
\widehat{\beta}^{(b)}_{\rm Lip} \;:=\;
\begin{cases}
\dfrac{\widehat{L}^{(b)}}{\widehat{m}^{(b)}(n+1)}, &
\text{if } \displaystyle \frac{1}{n+1}\le \min\{\widehat{m}^{(b)}\widehat{r}^{(b)},\widehat{\gamma}^{(b)}\}, \\[1.2em]
1, & \text{otherwise.}
\end{cases}
\]
Finally, aggregate by the mean:
\[
\widehat{\beta}_{\rm Lip} \;:=\; \frac{1}{B}\sum_{b=1}^B \widehat{\beta}^{(b)}_{\rm Lip}.
\]

\subsubsection{Empirical risk minimization}
For Proposition~\ref{prop:erm-stable-loss}, there is only one parameter to estimate: $\E[\rho(Z_{n+1})^2]$.
Given that it is an average over individual data points, we can simply use its plug-in estimator,
\begin{equation}
    \widehat{\rho^2} = \frac{1}{n}\sum\limits_{i=1}^n \max_{\theta, \theta' \in \Theta_G} \frac{|\ell(Z_i;\theta)-\ell(Z_i;\theta')|}{\|\theta - \theta'\|},
\end{equation}
for some grid $\Theta_G$, giving
\begin{equation}
    \widehat{\beta}_{\rm ERMLoss} = \frac{2\widehat{\rho^2}}{\lambda(n+1)}.
\end{equation}

Proposition~\ref{prop:erm-grad-stability} is more complex because $\E[\rho(Z_{n+1})\|\nabla\ell(Z_{n+1};\cA(D_{1:n}))\|_2]$ and $\E[\|\tfrac{1}{n}\sum_{i=1}^n\nabla\ell(Z_j;\cA(D_{1:n}))\|_2]$ depend on $D_{1:n+1}$.
We estimate them with the bootstrap by symmetrizing over the choice of the ``future'' point.
For each replicate $D^{(b)}=(Z^{(b)}_1,\ldots,Z^{(b)}_{n+1})$, and each $i\in[n{+}1]$, fit
\[
\hat\theta^{(b)}_{-i}\;:=\;\cA\!\bigl(D^{(b)}_{-i}\bigr),
\qquad
\widehat{\rho}^{(b)}_{i}\;:=\;\max_{\theta,\theta'\in\Theta_G}
\frac{\bigl|\ell(Z^{(b)}_{i};\theta)-\ell(Z^{(b)}_{i};\theta')\bigr|}{\|\theta-\theta'\|}.
\]
Define the per-replicate estimators of the two expectations appearing in
Proposition~\ref{prop:erm-grad-stability} by
\begin{equation}
G^{(b)}_1 := 
\frac{1}{n+1}\sum_{i=1}^{n+1}
\widehat{\rho}^{(b)}_{i}\,
\bigl\|\nabla\ell\!\bigl(Z^{(b)}_{i};\hat\theta^{(b)}_{-i}\bigr)\bigr\|_2 \text{ and } G^{(b)}_2 :=
\frac{1}{n+1}\sum_{i=1}^{n+1}
\Bigl\|
\frac{1}{n}\sum_{j\neq i}\nabla\ell\!\bigl(Z^{(b)}_{j};\hat\theta^{(b)}_{-i}\bigr)
\Bigr\|_2.
\end{equation}c
Aggregate by bootstrap means $\overline{G}_k=\frac{1}{B}\sum_{b=1}^B G^{(b)}_k$, and set
\[
\widehat{\beta}_{\rm ERMGrad}
\;:=\;
\min\Bigl\{\,1,\ \frac{2}{\lambda(n+1)}\bigl(\overline{G}_1+\overline{G}_2\bigr)\Bigr\}.
\]
This yields a plug-in (bootstrap) estimate of the upper bound from
Proposition~\ref{prop:erm-grad-stability}.

\section{Experiments}
\label{sec:experiments}
We perform two experiments to validate the method.
First, we run a real-data selective classification experiment on the Imagenet dataset.
Second, we run a simulation experiment to stress-test the selective classification method.

In each section, experiment with three methods. 
The first, which we call CRC-C, is the conservative version of conformal risk control, run at an adjusted level $\alpha'=\alpha-\beta$, where $\beta$ is the estimated algorithmic stability using the bootstrap.
The second, which we simply call CRC, is the unadjusted version of conformal risk control, i.e,~\eqref{eq:leftmost-root} with $\alpha$ set to be the desired risk level.
Finally, we run the Learn-then-Test procedure from~\cite{angelopoulos2025learn}, which provides a distribution-free guarantee that
\begin{equation}
    \P\left(\E\left[ \ell(X_{n+1}, Y_{n+1}; \thetahat) \big\vert D_{1:n} \right] \leq \alpha\right) \geq 1-\delta,
\end{equation}
where here we choose $\delta=0.1$.

\subsection{Selective Image Classification}

The Imagenet experiment follows the same setting as in~\cite{angelopoulos2023gentle}: we use a ResNet-152 as the base classifier and split the the highest probits from a ResNet-152 are used as $\hat{P}_1, \ldots, \hat{P}_{n+1}$.
The corresponding $\hat{Y}_1, \ldots, \hat{Y}_{n+1}$ represent class label predictions, and $Y_1, \ldots, Y_{n+1}$ represent the true labels.
We set $n=1000$.
See Figure~\ref{fig:selective-examples} for a description of the accuracy/prediction-rate tradeoff and examples of issued predictions and abstentions.

To assess the algorithms' performance, we display the thresholds chosen by all three algorithms and the resulting selective accuracy and prediction rate in Figure~\ref{fig:selective-imagenet-comparison}.
We also show density estimates of the selective accuracy and prediction rate in Figure~\ref{fig:selective-imagenet-kde}. 
The ideal algorithm issues as many predictions as possible subject to the accuracy constraint; that is, it should generally meet the accuracy constraint exactly, and maximize the prediction rate. CRC is the least conservative, LTT is the most (as it is a high-probability guarantee), and CRC-C is in the middle.
The estimated stability parameter is $\beta=0.007$, meaning CRC is essentially safe to use with no correction in all but the most sensitive practical circumstances.
\begin{figure}[p!]
    \centering
    \includegraphics[width=\linewidth]{figures/selective-classification-results.pdf}
    \caption{\textbf{Selective classification results on Imagenet.} The left plot shows the accuracy and prediction rate (the fraction of points for which a prediction is issued) as a function of $\theta$, the confidence threshold. The middle image shows a prediction that was issued, while the right image shows an abstention.}
    \label{fig:selective-examples}
    \includegraphics[width=\linewidth]{figures/conservative-crc-comparison.pdf}
    \caption{\textbf{Comparison of CRC-C, CRC, and LTT.} The left plot shows the choices of thresholds, LTT being the most conservative and CRC being the least by far. The right plot shows the selective accuracy and prediction rate induced by these thresholds in a single trial.}
    \label{fig:selective-imagenet-comparison}
    \includegraphics[width=\linewidth]{figures/conservative-crc-kde-distributions.pdf}
    \caption{\textbf{Distributions of selective accuracy and coverage for CRC-C, CRC, and LTT.} For each method, we show kernel density estimates of the selective accuracy and prediction rate in the left and right plots respectively. The densities are estimated using 100 random reshufflings of the calibration/validataion data split.}
    \label{fig:selective-imagenet-kde}
\end{figure}

\subsection{Simulations on selective classification}
Next, we stress-test the selective classification procedure in a simulation setting.
We simulate losses via the following model:
\begin{equation}
    \hat{P}_i \sim \mathrm{Unif}(0,1), \qquad E_i \sim \mathrm{Bern}\left(\epsilon \alpha + (1-\epsilon) \frac{1}{1 + e^{20(\hat{P}_i-0.5)}} \right), \qquad i=1, \ldots, n+1,
\end{equation}
for some instability parameter $\epsilon$.
The left panel of Figure~\ref{fig:selective-simulation} shows a single realization of the $n+1$-length empirical distribution with $n=500$, $\alpha=0.1$, and several choices of $\epsilon$.
For higher values of $\epsilon$, the risk hovers close to $\alpha$ for more values of $\theta$.
A higher $\epsilon$ therefore makes the selective prediction algorithm more unstable, as the algorithm chooses the lowest value of $\theta$ whose empirical risk is below $\alpha$, and this value could happen to land at many points. 
The right panel of the same figure shows the distribution of risks that results from running CRC-C, CRC, and LTT on the synthetic losses with $n=500$, $\epsilon=0.2$, and $\alpha=0.1$.
\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{figures/empirical_risk_instability.pdf}    \includegraphics[width=0.5\linewidth]{figures/risk_histogram_comparison.pdf}
    \caption{\textbf{Illustration of the data generating process and distribution of risks.} On the left we plot a single realization of the $n+1$-length empirical risk, with $n=500$ and $\epsilon=0.2$, according to the above model. On the right, we plot a histogram of the risks induced by different algorithms, over 100 trials, similarly to the left panel of Figure~\ref{fig:selective-imagenet-kde}.
    \label{fig:selective-simulation}}
\end{figure}

\section{Discussion}
This paper showed that the validity of conformal prediction can be viewed as a consequence of algorithmic stability, and that therefore, all stable algorithms benefit from conformal guarantees.
This expands the scope of the field, allowing us to prove distribution-free bounds easily for arbitrary tasks.
Finding such expert tasks and corresponding guarantees would be an exciting avenue for future work.

\begin{itemize}
    \item Full-conformal versions of this are easy.
    \item What about multicalibration?
    \item Connections to gradient equilibrium.
    \item Exactly recovering the argument of~\cite{gibbs2023conformal} using ERM.
    \item Tight stability bounds for certain statistical models, e.g., GLMs.
    \item Other connections to the stability literature?
\end{itemize}

\clearpage
\appendix

\section{Additional Proofs and Results}
\begin{proof}[Proof of Proposition~\ref{prop:general-stability}]
Fix $\epsilon\ge 0$ and define the random set
\begin{equation}
\mathcal{B}_\epsilon\ :=\ 
\Bigl\{\theta\in\Theta_m:
\ R(\theta)>\alpha+\epsilon\ \text{ and }\ 
\hat R_n(\theta)\le \alpha
\Bigr\}.
\end{equation}
Let $\thetahat:=\A(D_{1:n})$. Since $\hat R_n(\thetahat)\le \alpha$,
\begin{equation}
\{R(\thetahat)>\alpha+\epsilon\}\ \subseteq\ \{\mathcal{B}_\epsilon\neq\varnothing\},
\end{equation}
hence
\begin{equation}
\P\bigl(R(\thetahat)>\alpha+\epsilon\bigr)\ \leq\ \P(\mathcal{B}_\epsilon\neq\varnothing).
\end{equation}
Moreover, since $0\le R(\thetahat)\le 1$,
\begin{equation}
R(\thetahat)\ \leq\ \alpha+\epsilon+\ind{R(\thetahat)>\alpha+\epsilon}.
\end{equation}
Taking expectations gives
\begin{equation}
\E[R(\thetahat)]\ \leq\ \alpha+\epsilon+\P(\mathcal{B}_\epsilon\neq\varnothing).
\end{equation}

Now,
\begin{equation}
\{\mathcal{B}_\epsilon\neq\varnothing\}
=\bigcup_{\theta\in\Theta_m}
\Bigl\{R(\theta)>\alpha+\epsilon,\ \hat R_n(\theta)\le \alpha\Bigr\},
\end{equation}
so by a union bound,
\begin{equation}
\P(\mathcal{B}_\epsilon\neq\varnothing)
\ \leq\ 
\sum_{\theta\in\Theta_m}
\P\Bigl(R(\theta)>\alpha+\epsilon,\ \hat R_n(\theta)\le \alpha\Bigr).
\end{equation}
Fix $\theta\in\Theta_m$. If $R(\theta)\le \alpha+\epsilon$ the term is zero.  
If $R(\theta)>\alpha+\epsilon$, then
\begin{equation}
\P\Bigl(R(\theta)>\alpha+\epsilon,\ \hat R_n(\theta)\le \alpha\Bigr)
\ \leq\ 
\P\bigl(\hat R_n(\theta)-R(\theta)\le -\epsilon\bigr)
\ \leq\ 
\exp(-2n\epsilon^2),
\end{equation}
by Hoeffdingâ€™s inequality (for exchangeable random variables~\cite{foygel2024hoeffding}). Therefore,
\begin{equation}
\P(\mathcal{B}_\epsilon\neq\varnothing)
\ \leq\ 
(m+1)\exp(-2n\epsilon^2).
\end{equation}

Combining the bounds yields, for all $\epsilon \geq 0$,
\begin{equation}
\label{eq:general-bound-epsilon}
\E[R(\thetahat)]\ \leq\ \alpha+\epsilon+(m+1)\exp(-2n\epsilon^2).
\end{equation}
Since $Z_{n+1}$ is independent of $D_{1:n}$,
\begin{equation}
\E[\ell(Z_{n+1};\thetahat)]
=
\E[R(\thetahat)].
\end{equation}

Next, we optimize over $\epsilon \geq 0$.
The optimal $\epsilon^*$ of the right-hand side of~\eqref{eq:general-bound-epsilon} solves the first-order condition
\begin{align}
    \label{eq:general-bound-simplified}
    1+(m+1)(-4n\epsilon^*)\exp(-2n\epsilon^2)=0 & \Longleftrightarrow (m+1)\exp(-2n(\epsilon^*)^2) = \frac{1}{4n\epsilon^*} \\
    & \Longleftrightarrow -2ue^{-2u} = -\frac{1}{4n(m+1)^2} \\
    & \Longleftrightarrow -\frac{1}{2} W_{-1} \left( -\frac{1}{4n(m+1)^2}\right) = u \\
    & \Longleftrightarrow \epsilon^* = 
    \sqrt{-\frac{W_{-1} \left( -\frac{1}{4n(m+1)^2}\right)}{8n}}
\end{align}
where $u=2n(\epsilon^*)^2$.
Notice that~\eqref{eq:general-bound-simplified} gives the first inequality in the proposition statement.
It is known~\cite{loczi2020explicit} that $W_{-1}(x) \geq \frac{e\ln(-x)}{e-1}$ for $x \in [-1/e, 0)$, allowing us to write
\begin{equation}
    \epsilon^* \leq 
    \sqrt{-\frac{W_{-1} \left( -\frac{1}{4n(m+1)^2}\right)}{8n}} \leq \sqrt{\frac{e\ln(4n(m+1)^2)}{8(e-1)n}}.
\end{equation}
In the same range, $W_{-1}(x) \leq \ln(-x)-\ln(-\ln(-x))$, yielding
\begin{equation}
    \epsilon^* \geq \sqrt{\frac{\ln(4n(m+1)^2)-\ln(\ln(4n(m+1)^2))}{8n}}.
\end{equation}
Plugging in these bounds into~\eqref{eq:general-bound-simplified} completes the proof.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:erm-stable-loss}]
Write $\thetahat:=\A(D_{1:n+1})$ and $\thetahat_{-i}:=\A(D_{-i})$.
By~\ref{eq:lipschitz-rho-simple}, we have that
\begin{equation}
    \frac{1}{n+1}\sum_{i=1}^{n+1}\left(\ell(Z_i;\thetahat_{-i}) - 
    \ell(Z_i;\thetahat)\right) \leq \frac{1}{n+1}\sum_{i=1}^{n+1} \rho(Z_i)\| \thetahat - 
    \thetahat_{-i} \|_2.
\end{equation}
The remainder of this proof will show that the following upper-bound holds:
\begin{equation}
    \label{eq:erm-rho-ub}
    \frac{1}{n+1}\sum_{i=1}^{n+1} \rho(Z_i)\| \thetahat - 
    \thetahat_{-i} \|_2. \leq \frac{2}{\lambda (n+1)^2}\sum_{i=1}^{n+1}\rho(Z_i)^2,
\end{equation}
after which taking expectations reveals that
\begin{equation}
    \E\left[\frac{1}{n+1}\sum_{i=1}^{n+1}\left(\ell(Z_i;\thetahat_{-i}) - 
    \ell(Z_i;\thetahat)\right)\right] \leq \frac{2}{\lambda (n+1)^2}\sum_{i=1}^{n+1}\E\left[\rho(Z_i)^2\right] = \frac{2\E\left[\rho(Z_1)^2\right]}{\lambda (n+1)},
\end{equation}
proving the result.



Towards proving~\eqref{eq:erm-rho-ub}, we will upper-bound $\|\thetahat - \thetahat_{-i}\|$ in terms of $\rho(Z_i)$. We will make use of the fact that for any $g\in\partial_\theta \ell(z;\theta)$,
\begin{equation}
    \label{eq:subgradient-rho}
    \ell(z;\theta)+\|g\|_2^2. \leq \ell(z;\theta+g) \leq \ell(z;\theta)+\rho(z)\|g\|_2 \implies \|g\|_2 \leq \rho(z)
\end{equation}
so .
Define the (strongly convex) objectives
\begin{equation}
F(\theta)\ :=\ \hat R_{D_{1:n+1}}(\theta)+\frac{\lambda}{2}\|\theta\|_2^2,
\qquad
F_{-i}(\theta)\ :=\ \hat R_{D_{-i}}(\theta)+\frac{\lambda}{2}\|\theta\|_2^2.
\end{equation}
Since $0\in\partial F(\thetahat)$, there exist $g_1,\ldots,g_{n+1}$ with $g_j\in\partial_\theta \ell(Z_j;\thetahat)$ such that
\begin{equation}
\label{eq:opt-full}
\frac{1}{n+1}\sum_{j=1}^{n+1} g_j\ +\ \lambda \thetahat\ =\ 0.
\end{equation}
Define
\begin{equation}
s_{-i}\ :=\ \frac{1}{n}\sum_{j\neq i} g_j\ +\ \lambda \thetahat.
\end{equation}
Since $\hat R_{D_{-i}}$ is an average of convex functions, $\frac{1}{n}\sum_{j\neq i}g_j\in\partial \hat R_{D_{-i}}(\thetahat)$, hence $s_{-i}\in\partial F_{-i}(\thetahat)$.
Using~\eqref{eq:opt-full},
\begin{equation}
s_{-i}
=
\frac{1}{n}\sum_{j\neq i} g_j\ -\ \frac{1}{n+1}\sum_{j=1}^{n+1} g_j
=
\frac{1}{(n+1)n}\sum_{j\neq i} g_j\ -\ \frac{1}{n+1}g_i.
\end{equation}

We now use the strong convexity of $F_{-i}$.
Take any $u,v\in\R^d$, and any $a\in\partial F_{-i}(u)$ and $b\in\partial F_{-i}(v)$.
Then $a=a_0+\lambda u$ and $b=b_0+\lambda v$ for some $a_0\in\partial\hat{R}_{D_{-i}}(u)$ and $b_0\in\partial\hat{R}_{D_{-i}}(v)$.
By monotonicity of the subdifferetial operator, $\langle a_0-b_0,\ u-v\rangle\ \ge\ 0$, so
\begin{equation}
\label{eq:strong-mono}
\langle a-b,\ u-v\rangle
=
\langle a_0-b_0,\ u-v\rangle + \lambda\|u-v\|_2^2
\ \ge\ \lambda\|u-v\|_2^2.
\end{equation}
Applying~\eqref{eq:strong-mono} with $u=\thetahat$, $v=\thetahat_{-i}$, $a=s_{-i}\in\partial F_{-i}(\thetahat)$, and $b=0\in\partial F_{-i}(\thetahat_{-i})$ gives
\begin{equation}
\lambda\|\thetahat-\thetahat_{-i}\|_2^2\ \le\ \langle s_{-i},\ \thetahat-\thetahat_{-i}\rangle
\ \le\ \|s_{-i}\|_2\,\|\thetahat-\thetahat_{-i}\|_2,
\end{equation}
hence, by~\eqref{eq:subgradient-rho}, 
\begin{align}
\label{eq:param-shift}
\rho(Z_i)\|\thetahat-\thetahat_{-i}\|_2\ & \leq \frac{\rho(Z_i)}{\lambda}\|s_{-i}\|_2 \\
& = \frac{\rho(Z_i)}{\lambda}\left\|\frac{1}{(n+1)n}\sum_{j\neq i} g_j\ -\ \frac{1}{n+1}g_i\right\|_2 \\
& \leq \frac{1}{\lambda (n+1)}\left(\rho(Z_i)^2+\frac{1}{n}\sum_{j\neq i}\rho(Z_i)\rho(Z_j)\right).
\end{align}
Averaging over $i=1,\ldots,n+1$ yields
\begin{align}
\label{eq:avg-diff}
\frac{1}{n+1}\sum_{i=1}^{n+1}\rho(Z_i)\|\thetahat-\thetahat_{-i}\|_2\
&\le
\frac{1}{\lambda n+1}\left(
\frac{1}{n+1}\sum_{i=1}^{n+1}\rho(Z_i)^2
+\frac{1}{(n+1)n}\sum_{i\neq j}\rho(Z_i)\rho(Z_j)
\right).
\end{align}
Let $S:=\sum_{i=1}^{n+1}\rho(Z_i)$ and $Q:=\sum_{i=1}^{n+1}\rho(Z_i)^2$.
Then $\sum_{i\neq j}\rho(Z_i)\rho(Z_j)=S^2-Q$, so the second term in~\eqref{eq:avg-diff} equals $(S^2-Q)/((n+1)n)$.
By Cauchy--Schwarz, $S^2\le (n+1)Q$, hence
\begin{equation}
\frac{S^2-Q}{(n+1)n}\ \le\ \frac{(n+1)Q-Q}{(n+1)n}\ =\ \frac{Q}{n+1}.
\end{equation}
Plugging this into~\eqref{eq:avg-diff} gives the desired bound in~\eqref{eq:erm-rho-ub}.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:erm-grad-stability}]
Define
\begin{equation}
F_{n+1}(\theta) = \frac{1}{n+1}\sum_{j=1}^{n+1} \ell(z_j;\theta) + \frac{\lambda}{2}\|\theta\|_2^2,
\qquad
\hat{\theta}_{n+1}\in\arg\min_{\theta}F_{n+1}(\theta)=\A(D_{1:n+1}),
\end{equation}
and $F_{-i},\hat{\theta}_{-i}$ accordingly for all $i$. Since $\ell(\cdot;\theta)$ is convex in $\theta$, $F_{n+1}$ and $F_{-i}$ are $(\mu+\lambda)$-strongly convex.
This implies that for any $u\in\R^d$, $\|\hat{\theta}_{n+1}-u\|_2 \leq \frac{1}{\mu+\lambda}\|F_{D_{1:n+1}}(u)\|$
Applying this with $u=\hat{\theta}_{-i}$ gives
\begin{equation}
\|\hat{\theta}_{n+1}-\hat{\theta}_{-i}\|_2
\leq \frac{1}{\mu+\lambda}\|\nabla F_{D_{1:n+1}}(\hat{\theta}_{-i})\|_2 = \frac{1}{\mu+\lambda}\|\nabla F_{D_{1:n+1}}(\hat{\theta}_{-i}) - \nabla F_{-i}(\hat{\theta}_{-i})\|_2,
\end{equation}
where in the last step we used that $0 = \nabla F_{-i}(\hat{\theta}_{-i})$. 
Next, we use the fact that 
\begin{multline}
\nabla F_{D_{1:n+1}}(\hat{\theta}_{-i}) - \nabla F_{-i}(\hat{\theta}_{-i}) = \frac{1}{n+1}\nabla\ell(Z_i;\hat{\theta}_{-i}) + \Big(\frac{1}{n+1}-\frac{1}{n}\Big)\sum_{j\neq i} \nabla\ell(Z_j;\hat{\theta}_{-i}) \\
= \frac{1}{n+1}\nabla\ell(Z_i;\hat{\theta}_{-i}) - \frac{1}{n(n+1)}\sum_{j\neq i} \nabla\ell(Z_j;\hat{\theta}_{-i}),
\end{multline}
so $\|\hat{\theta}_{n+1}-\hat{\theta}_{-i}\|_2 \leq \tfrac{1}{n+1}\nabla\ell(Z_i;\hat{\theta}_{-i}) - \tfrac{1}{n(n+1)}\sum_{j\neq i} \nabla\ell(Z_j;\hat{\theta}_{-i})$.

Next, picking $i=n+1$ and using $\rho(z)$-Lipschitzness,
\begin{multline}
\|\nabla\ell(Z_{n+1};\hat{\theta}_{-(n+1)})-\nabla\ell(Z_{n+1};\hat{\theta}_{n+1})\|_2
\leq
\rho(Z_{n+1})\|\hat{\theta}_{n+1}-\hat{\theta}_{-(n+1)}\|_2
\\
\leq \frac{\rho(Z_{n+1})\|\nabla\ell(Z_{n+1};\hat{\theta}_{-(n+1)})\|_2}{(\mu+\lambda)(n+1)} + \frac{\rho(Z_{n+1})\|\sum_{j=1}^{n} \nabla\ell(Z_j;\hat{\theta}_{-(n+1)})\|_2}{(\mu+\lambda) n(n+1)}.
\end{multline}
Taking expectations gives
\begin{multline}
\E\left[\|\nabla\ell(Z_{n+1};\hat{\theta}_{-(n+1)})-\nabla\ell(Z_{n+1};\hat{\theta}_{n+1})\|_2\right] \\
\leq \frac{\E\left[\rho(Z_{n+1})\|\nabla\ell(Z_{n+1};\hat{\theta}_{-(n+1)})\|_2\right]}{(\mu+\lambda)(n+1)} + \frac{\E\left[\rho(Z_{n+1})\|\tfrac{1}{n}\sum_{j=1}^{n} \nabla\ell(Z_j;\hat{\theta}_{-(n+1)})\|_2\right]}{(\mu+\lambda) (n+1)}.
\end{multline}
Finally, using norm inequalities gives
\begin{align}
    & \E\left[\nabla\ell(Z_{n+1};\hat{\theta}_{-(n+1)}) - \nabla\ell(Z_{n+1};\hat{\theta}_{n+1})\right] \\
    \preceq &\| \E\left[\nabla\ell(Z_{n+1};\hat{\theta}_{-(n+1)}) - \nabla\ell(Z_{n+1};\hat{\theta}_{n+1})\right] \|_{\infty} \mathbf{1}_d \\
    \preceq &\E\left[\|\nabla\ell(Z_{n+1};\hat{\theta}_{-(n+1)}) - \nabla\ell(Z_{n+1};\hat{\theta}_{n+1})\|_2\right] \mathbf{1}_d  \\
    \preceq & \frac{\E\left[\rho(Z_{n+1})\|\nabla\ell(Z_{n+1};\hat{\theta}_{-(n+1)})\|_2\right] + \E\left[\rho(Z_{n+1})\|\tfrac{1}{n}\sum_{j=1}^{n} \nabla\ell(Z_j;\hat{\theta}_{-(n+1)})\|_2\right]}{(\mu+\lambda)(n+1)}\mathbf{1}_d,
\end{align}
implying the desired conclusions.
\end{proof}
\begin{proof}[Proof of Corollary~\ref{cor:regularized-erm}]
    First notice that
    \begin{equation}
        \frac{1}{n+1}\sum\limits_{i=1}^{n+1}\nabla\ell(X_{i}, Y_{i}; \A(D_{1:n+1})) + \lambda\A(D_{1:n+1}) = 0,
    \end{equation}
    so by exchangeability,
    \begin{equation}
        \E\left[\nabla\ell(Z_{n+1},\A(D_{1:n+1})) + \lambda\A(D_{1:n+1})\right] = \E\left[ \frac{1}{n+1}\sum\limits_{i=1}^{n+1}\nabla\ell(X_{i}, Y_{i}; \A(D_{1:n+1})) + \lambda\A(D_{1:n+1}) \right] = 0.
    \end{equation}
    Combining Proposition~\ref{prop:erm-grad-stability} and Theorem~\ref{thm:main-high-d} therefore gives that
    \begin{equation}
        \E\left[\nabla\ell(Z_{n+1},\A(D_{1:n+1})) + \lambda\A(D_{1:n})\right] \preceq \beta \Longleftrightarrow \E\left[\nabla\ell(Z_{n+1},\A(D_{1:n+1}))\right] \preceq \beta - \lambda\E[\A(D_{1:n})]
    \end{equation}
\end{proof}
\begin{proof}[Proof of Proposition~\ref{prop:regularized-erm-conservative}]
    The same argument as Corollary~\ref{cor:regularized-erm} applied to the loss function $\ell(z;\theta) + \gamma\mathbf{1}_d^\top\theta$ gives that
    \begin{equation}
        \E\left[\nabla\ell(X_{n+1}, Y_{n+1}; \A(D_{1:n})) \right] \preceq \beta_\gamma + \lambda\E[\A(D_{1:n})] - \gamma\mathbf{1}_d.
    \end{equation}
    The value of $\gamma$ that makes the right-hand side zero is
    \begin{equation}
        \gamma
        =
        \frac{
        \displaystyle
        \frac{
        \E\left[\rho(Z_{n+1})\|\nabla\ell(Z_{n+1};\hat{\theta}_{-(n+1)})\|_2\right]
        +
        \E[\rho(Z_{n+1})]\,
        \E\left[\left\|\frac{1}{n}\sum_{j=1}^n \nabla\ell(Z_j;\hat{\theta}_{-(n+1)})\right\|_2\right]
        }{(\mu+\lambda)(n+1)}
        +
        \lambda\left\|\E[\A(D_{1:n})]\right\|_\infty
        }{
        \displaystyle
        1 - \frac{2\,\E[\rho(Z_{n+1})]}{(\mu+\lambda)(n+1)}
        },
    \end{equation}
    provided $\lambda(n+1) > 2 \E[\rho(Z_{n+1})]$.
\end{proof}

\begin{proof}[Proof of Corollary~\ref{cor:unbiased-least-squares}]
    Let $\thetahat_n=\A(D_{1:n})$. Here $\mu=1$, $\lambda=0$, and $\rho(Z_{n+1}) = 1$, so Corollary~\ref{cor:regularized-erm} gives
    \begin{multline}
        \E[X_{n+1}^\top(f(X_{n+1})+X_{n+1}^\top\thetahat_n - Y_{n+1})] \preceq \frac{\E[\|X_{n+1}(f(X_{n+1})+X_{n+1}^\top\thetahat_n - Y_{n+1})\|_2]}{n+1} \\
        = \frac{\E[|f(X_{n+1})+X_{n+1}^\top\thetahat_n - Y_{n+1})|]}{n+1}. 
    \end{multline}
    Since $X_{n+1} \in \{0,1\}^d$, the inequality implies that for the $j$th coordinate,
    \begin{equation}
        \E[X_{n+1,j}(f(X_{n+1})+X_{n+1}^\top\thetahat_n - Y_{n+1})] = \P(X_{n+1,j}=1)\E[(f(X_{n+1})+\thetahat_{n,j} - Y_{n+1})\mid X_{n+1,j}=1]
    \end{equation}
    Therefore,
    \begin{equation}
        \E[Y_{n+1}\mid X_{n+1,j}=1] \geq \E[f(X_{n+1})+\thetahat_{n,j} \mid X_{n+1,j}=1] - \frac{\E[|f(X_{n+1})+X_{n+1}^\top\thetahat_n - Y_{n+1})|]}{\P(X_{n+1,j}=1)(n+1)}.
    \end{equation}
    The symmetric argument (with all inequalities flipped, beginning with Theorem~\ref{thm:main-high-d}) gives that
    \begin{equation}
        \E[Y_{n+1}\mid X_{n+1,j}=1] \leq \E[f(X_{n+1})+\thetahat_{n,j} \mid X_{n+1,j}=1] + \frac{\E[|f(X_{n+1})+X_{n+1}^\top\thetahat_n - Y_{n+1})|]}{\P(X_{n+1,j}=1)(n+1)}.
    \end{equation}
\end{proof}

\bibliographystyle{alpha}
\bibliography{bibliography}

\end{document}