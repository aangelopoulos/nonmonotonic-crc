{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Add parent directory to system path\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from erm import RegularizedERM, ERMStabilityEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the COMPAS dataset\n",
    "df = pd.read_csv('./raw_data/compas-scores-two-years.csv')\n",
    "df = df[['compas_screening_date', 'sex', 'race', 'v_decile_score', 'is_recid']]\n",
    "df = df[df.race.isin([\"African-American\", \"Caucasian\", \"Hispanic\"])]\n",
    "df['phat'] = df['v_decile_score']/10.0\n",
    "df.compas_screening_date = pd.to_datetime(df.compas_screening_date)\n",
    "df = df.sort_values(by='compas_screening_date')\n",
    "\n",
    "# Prepare tensors\n",
    "y = df.is_recid.to_numpy().astype(float)\n",
    "yhat = df.phat.to_numpy().astype(float)\n",
    "# concatenate race and sex dummies\n",
    "dummy_df_races = pd.get_dummies(df.race)\n",
    "dummy_df_sexes = pd.get_dummies(df.sex)\n",
    "dummy_df = pd.concat([dummy_df_races, dummy_df_sexes], axis=1)\n",
    "races = dummy_df_races.values.astype(float)\n",
    "sexes = dummy_df_sexes.values.astype(float)\n",
    "order_races = dummy_df_races.columns.values.tolist()\n",
    "order_sexes = dummy_df_sexes.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLS:\n",
    "    \"\"\"\n",
    "    Ordinary Least Squares regression using RegularizedERM with optional conservative guarantee.\n",
    "    \n",
    "    Solves: min_θ (1/2n) ||y - Xθ||² + (λ/2)||θ||² - γ 1_d^T θ\n",
    "    \n",
    "    The linear term -γ 1_d^T θ provides a conservative gradient guarantee (Proposition regularized-erm-conservative).\n",
    "    \n",
    "    Provides vectorized fit and predict methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lam=0.01, gamma=0.0):\n",
    "        \"\"\"\n",
    "        Initialize OLS regressor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lam : float\n",
    "            Regularization parameter (ridge penalty)\n",
    "        gamma : float or np.ndarray\n",
    "            Linear term coefficient for conservative guarantee.\n",
    "            If float: same gamma for all coordinates.\n",
    "            If array: per-coordinate gamma (length d).\n",
    "        \"\"\"\n",
    "        self.lam = lam\n",
    "        self.gamma = gamma\n",
    "        self.theta = None\n",
    "        self.erm = None\n",
    "    \n",
    "    def _loss_fn(self, data, theta):\n",
    "        \"\"\"\n",
    "        Squared error loss: (1/2)(y - X^T θ)²\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : tuple or array\n",
    "            If tuple: (X, y) where X is (n, d) and y is (n,)\n",
    "            If array: assumes structure is data[:, :-1] = X, data[:, -1] = y\n",
    "        theta : np.ndarray\n",
    "            Parameter vector (d,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss : np.ndarray\n",
    "            Loss for each sample (n,)\n",
    "        \"\"\"\n",
    "        if isinstance(data, tuple):\n",
    "            X, y = data\n",
    "        else:\n",
    "            X = data[:, :-1]\n",
    "            y = data[:, -1]\n",
    "        \n",
    "        predictions = X @ theta\n",
    "        return 0.5 * (y - predictions) ** 2\n",
    "    \n",
    "    def _grad_fn(self, data, theta):\n",
    "        \"\"\"\n",
    "        Gradient of squared error: -(y - X^T θ) X\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : tuple or array\n",
    "            If tuple: (X, y) where X is (n, d) and y is (n,)\n",
    "            If array: assumes structure is data[:, :-1] = X, data[:, -1] = y\n",
    "        theta : np.ndarray\n",
    "            Parameter vector (d,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        gradient : np.ndarray\n",
    "            Gradient for each sample (n, d)\n",
    "        \"\"\"\n",
    "        if isinstance(data, tuple):\n",
    "            X, y = data\n",
    "        else:\n",
    "            X = data[:, :-1]\n",
    "            y = data[:, -1]\n",
    "        \n",
    "        predictions = X @ theta\n",
    "        residuals = y - predictions\n",
    "        return -(residuals[:, np.newaxis] * X)\n",
    "    \n",
    "    def _loss_fn_conservative(self, theta, data):\n",
    "        \"\"\"\n",
    "        Regularized empirical risk with linear term: R̂_D(θ) + (λ/2)||θ||²_2 - γ 1_d^T θ\n",
    "        \"\"\"\n",
    "        return self._loss_fn(data, theta) - np.sum(self.gamma * theta)  # -γ 1_d^T θ\n",
    "    \n",
    "    def _gradient_conservative(self, theta, data):\n",
    "        \"\"\"\n",
    "        Gradient with linear term: ∇R̂_D(θ) + λθ - γ 1_d\n",
    "        \"\"\"\n",
    "        return self._grad_fn(data, theta) - self.gamma\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit OLS model to training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Feature matrix (n, d)\n",
    "        y : np.ndarray\n",
    "            Target vector (n,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : OLS\n",
    "            Fitted model\n",
    "        \"\"\"\n",
    "        # Store data as tuple for loss/grad functions\n",
    "        data = (X, y)\n",
    "        \n",
    "        # Initialize theta\n",
    "        d = X.shape[1]\n",
    "        theta_init = np.zeros(d)\n",
    "                \n",
    "        # If gamma is non-zero, use custom optimization with conservative objective\n",
    "        self.erm = RegularizedERM(lam=self.lam, theta_init=theta_init)\n",
    "        self.theta = self.erm.fit(data, self._loss_fn, self._grad_fn)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Feature matrix (n, d)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        predictions : np.ndarray\n",
    "            Predicted values (n,)\n",
    "        \"\"\"\n",
    "        if self.theta is None:\n",
    "            raise ValueError(\"Model has not been fitted yet. Call fit() first.\")\n",
    "        \n",
    "        return X @ self.theta\n",
    "\n",
    "\n",
    "def precompute_beta_ols(X, y, lam=0.01, n_bootstrap=100):\n",
    "    \"\"\"\n",
    "    Precompute stability parameter β for OLS using direct estimation.\n",
    "    \n",
    "    Uses ERMStabilityEstimator.estimate_beta_loss_direct() which implements:\n",
    "    Δ^(b) = (1/(n+1)) Σ_i [ℓ(Z_i^(b); A(D_{-i}^(b))) - ℓ(Z_i^(b); A*(D^(b)))]\n",
    "    β̂ = E[Δ]_+\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix (n, d)\n",
    "    y : np.ndarray\n",
    "        Target vector (n,)\n",
    "    lam : float\n",
    "        Regularization parameter\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap replicates\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    beta_hat : float\n",
    "        Estimated stability parameter\n",
    "    \"\"\"\n",
    "    # Create OLS instance to use its loss and gradient functions\n",
    "    theta_init = np.zeros(X.shape[1])\n",
    "    ols = OLS(lam=lam, gamma=0.0)\n",
    "    \n",
    "    # Prepare data as concatenated array for ERMStabilityEstimator\n",
    "    data = np.concatenate([X, y[:, np.newaxis]], axis=1)\n",
    "    \n",
    "    # Create stability estimator\n",
    "    estimator = ERMStabilityEstimator(lam=lam, n_bootstrap=n_bootstrap)\n",
    "    \n",
    "    # Estimate beta using direct method with OLS loss and gradient functions\n",
    "    beta_hat = estimator.estimate_beta_loss_direct(\n",
    "        data=data,\n",
    "        loss_fn=ols._loss_fn,\n",
    "        grad_fn=ols._grad_fn,\n",
    "        theta_init=theta_init\n",
    "    )\n",
    "    \n",
    "    return beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Standard OLS (gamma = 0)\n",
      "============================================================\n",
      "Fitted coefficients (θ): [ 1.42193893e-01  4.71339092e-01  1.01223399e-01  4.13978390e-02\n",
      " -4.27345235e-04  2.85793372e-02  1.13614555e-01]\n",
      "Mean squared error: 0.2260\n",
      "R²: 0.0954\n",
      "\n",
      "============================================================\n",
      "Precomputing β (stability parameter)\n",
      "============================================================\n",
      "Note: This uses bootstrap with leave-one-out. May take a few minutes...\n",
      "Estimated β: 0.000121\n",
      "\n",
      "============================================================\n",
      "Conservative OLS (gamma = β = 0.000121)\n",
      "============================================================\n",
      "Fitted coefficients (θ): [ 1.42193893e-01  4.71339092e-01  1.01223399e-01  4.13978390e-02\n",
      " -4.27345235e-04  2.85793372e-02  1.13614555e-01]\n",
      "Mean squared error: 0.2260\n",
      "R²: 0.0954\n",
      "\n",
      "============================================================\n",
      "Comparison\n",
      "============================================================\n",
      "Difference in coefficients (L2): 0.000000\n",
      "Max absolute difference in coefficients: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Example usage of OLS module\n",
    "# Create design matrix: intercept + yhat + race dummies + sex dummies\n",
    "X = np.column_stack([\n",
    "    np.ones(len(y)),  # intercept\n",
    "    yhat,             # compas prediction\n",
    "    races,            # race dummies\n",
    "    sexes             # sex dummies\n",
    "])\n",
    "\n",
    "# Standard OLS (no conservative guarantee)\n",
    "print(\"=\" * 60)\n",
    "print(\"Standard OLS (gamma = 0)\")\n",
    "print(\"=\" * 60)\n",
    "ols_standard = OLS(lam=0.01, gamma=0.0)\n",
    "ols_standard.fit(X, y)\n",
    "y_pred_standard = ols_standard.predict(X)\n",
    "\n",
    "print(f\"Fitted coefficients (θ): {ols_standard.theta}\")\n",
    "print(f\"Mean squared error: {np.mean((y - y_pred_standard)**2):.4f}\")\n",
    "print(f\"R²: {1 - np.sum((y - y_pred_standard)**2) / np.sum((y - y.mean())**2):.4f}\")\n",
    "\n",
    "# Precompute beta for OLS (this may take a while with bootstrap)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Precomputing β (stability parameter)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Note: This uses bootstrap with leave-one-out. May take a few minutes...\")\n",
    "beta_hat = precompute_beta_ols(X, y, lam=0.01, n_bootstrap=10)  # Fewer bootstraps for demo\n",
    "print(f\"Estimated β: {beta_hat:.6f}\")\n",
    "\n",
    "# Conservative OLS with gamma = beta\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Conservative OLS (gamma = β = {beta_hat:.6f})\")\n",
    "print(\"=\" * 60)\n",
    "ols_conservative = OLS(lam=0.01, gamma=beta_hat)\n",
    "ols_conservative.fit(X, y)\n",
    "y_pred_conservative = ols_conservative.predict(X)\n",
    "\n",
    "print(f\"Fitted coefficients (θ): {ols_conservative.theta}\")\n",
    "print(f\"Mean squared error: {np.mean((y - y_pred_conservative)**2):.4f}\")\n",
    "print(f\"R²: {1 - np.sum((y - y_pred_conservative)**2) / np.sum((y - y.mean())**2):.4f}\")\n",
    "\n",
    "# Compare predictions\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Difference in coefficients (L2): {np.linalg.norm(ols_conservative.theta - ols_standard.theta):.6f}\")\n",
    "print(f\"Max absolute difference in coefficients: {np.max(np.abs(ols_conservative.theta - ols_standard.theta)):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
